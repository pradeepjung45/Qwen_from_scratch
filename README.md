# ğŸ”¥ Qwen_from_scratch: Building a Modern LLM from the Ground Up

> ğŸ§  A fully implemented small-scale language model inspired by **Qwen3**, built **from scratch** using PyTorch.  
> Every component â€” from **GQA** to **RoPE**, **SwiGLU**, **Muon optimizer**, and **weight tying** â€” is explained and implemented in pure code.

ğŸ¯ **Purpose**: Learn how real large language models work â€” not just use them.

ğŸš€ Trained on 500K tokens | ğŸ“Š Final Perplexity: **3.05** | ğŸ’¾ Only **32M parameters**

ğŸ”— Inspired by Qwen3 architecture | âœ… Educational focus | ğŸš€ Production-ready training loop

---

## ğŸŒŸ Features & Modern Techniques

This repo implements **state-of-the-art components** used in todayâ€™s top LLMs:

| Feature | Description |
|--------|-------------|
| âœ… **Grouped-Query Attention (GQA)** | Shared KV heads â†’ faster inference, less memory |
| âœ… **Rotary Position Embeddings (RoPE)** | Relative position encoding, works on longer sequences |
| âœ… **SwiGLU Feed-Forward** | More expressive than ReLU, used in LLaMA-3, Qwen |
| âœ… **RMSNorm** | Faster, simpler alternative to LayerNorm |
| âœ… **Muon Optimizer** | Momentum + orthogonalization via Newton-Schulz iteration |
| âœ… **Hybrid Optimization** | Muon for 2D weights, AdamW for embeddings/norms |
| âœ… **Automatic Mixed Precision (AMP)** | Train with `bfloat16` â†’ 2x speed |
| âœ… **Gradient Accumulation** | Simulate larger batch sizes |
| âœ… **Weight Tying** | Share input/output embeddings â†’ better generalization |
| âœ… **Text Generation** | With temperature, top-k, top-p sampling |

Perfect for learning, research, or prototyping.

---

## ğŸ“¦ Model Architecture Summary

| Parameter | Value |
|---------|-------|
| **Model Size (`d_model`)** | 384 |
| **Number of Layers (`n_layers`)** | 6 |
| **Attention Heads (`n_heads`)** | 8 |
| **KV Heads (GQA)** | 4 |
| **FFN Hidden Dim (`d_ff`)** | 1536 |
| **Max Sequence Length** | 512 |
| **Vocab Size** | ~50k (SmolLM tokenizer) |
| **Total Parameters** | 32.15M |
| **Trained On** | 500K tokens from SmolLM corpus |
| **Training Steps** | 2000 |

Despite its tiny size, it learns meaningful language patterns.

---

## ğŸ“Š Training Results

After **2000 steps** (~12.6 minutes on Tesla T4), the model achieved excellent performance:

| Metric | Final Value |
|-------|-------------|
| **Validation Loss** | 1.1146 |
| **Validation Accuracy** | 75.12% |
| **Validation Perplexity** | **3.05** âœ… |

> ğŸ” Best model saved at step 1500 with PPL = 6.67

These are **outstanding results** for a 32M-parameter model trained on limited data.

---

## ğŸš€ How to Run

### 1. Clone the Repo
```bash
git clone https://github.com/pradeepjung45/Qwen_from_scratch.git
cd Qwen_from_scratch
